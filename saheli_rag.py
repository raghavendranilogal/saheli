# -*- coding: utf-8 -*-
"""SAHELI_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YCDCZpyzjwCMtCZrVxBuXm0aV-E-xugI
"""

#pip list

#!pip install SpeechRecognition pyttsx3 pyaudio sounddevice

#!pip install PyPDF2 faiss-cpu

import os
import google.generativeai as genai
import PyPDF2
import faiss
import numpy as np
import gc
import torch
from transformers import AutoTokenizer, AutoModel


genai.configure(api_key="AIzaSyBJg2ka_TGtimcK6EtGEsW6j7DwVa8hgZw")

# Initialize Gemini model
model = genai.GenerativeModel('gemini-2.5-pro-exp-03-25')





#model = genai.GenerativeModel('gemini-1.5-pro-latest')


#prompt = "Hello! Can you tell who is  maheshbabu?"

 #response = model.generate_content(prompt)

#rint(response.text)

# Get file path from user input
print ("starting to read file")
file_path = "/Users/sabyasachi.upadhy/Documents/SAHELI/data.pdf"

# Function to extract text from PDF
#print("Checking the status of file read" + file_path.name)

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

# Function to extract text from TXT
def extract_text_from_txt(txt_path):
    with open(txt_path, 'r', encoding='utf-8') as file:
        return file.read()

# Detect file type and extract text
if file_path.lower().endswith('.pdf'):
    document_text = extract_text_from_pdf(file_path)
elif file_path.lower().endswith('.txt'):
    document_text = extract_text_from_txt(file_path)
else:
    raise ValueError("Unsupported file type. Please upload a .pdf or .txt file.")

#print("Extracted document text preview:")
#rint(document_text[:1000])  # preview first 1000 characters

# Save extracted text to a file


#print("Document text saved as 'document.txt'")

# Split the document into chunks
def split_text(text, max_tokens=500):
    sentences = text.split('. ')
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk.split()) + len(sentence.split()) <= max_tokens:
            current_chunk += sentence + ". "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

chunks = split_text(document_text)
print(f"Total chunks created: {len(chunks)}")
print("Sample chunk:", chunks[0])

def get_embeddings(texts, model_name='distilbert-base-uncased'):
    # Initialize tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    # Set model to evaluation mode and move to CPU
    model.eval()
    model = model.to('cpu')
    
    # Process texts in smaller batches to manage memory
    batch_size = 4  # Reduced batch size
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        try:
            # Tokenize the batch
            encoded = tokenizer(batch, padding=True, truncation=True, max_length=64, return_tensors='pt')
            encoded = {k: v.to('cpu') for k, v in encoded.items()}  # Move to CPU
            
            # Generate embeddings
            with torch.no_grad():
                outputs = model(**encoded)
                # Use mean pooling
                batch_embeddings = outputs.last_hidden_state.mean(dim=1)
                embeddings.extend(batch_embeddings.numpy())
            
            # Clear memory
            del encoded
            del outputs
            del batch_embeddings
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            gc.collect()
            
        except Exception as e:
            print(f"Error processing batch {i//batch_size}: {str(e)}")
            continue
    
    # Clean up
    del model
    del tokenizer
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    gc.collect()
    
    return np.array(embeddings)

# Generate embeddings for chunks
try:
    print("Generating embeddings...")
    embeddings = get_embeddings(chunks)
    print(f"Total embeddings created: {len(embeddings)}")
    print("Sample embedding vector:", embeddings[0][:5])
    
    # Initialize FAISS index
    embedding_dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(embedding_dim)
    index.add(embeddings)
    
    print(f"FAISS index contains {index.ntotal} vectors")
    
    # Clear memory
    del embeddings
    gc.collect()
    
except Exception as e:
    print(f"Error occurred: {str(e)}")
    gc.collect()
    torch.cuda.empty_cache() if torch.cuda.is_available() else None

# Test query
#query = "What is the main idea of the document?"
#query_embedding = embedding_model.encode([query])

# Search in FAISS index
#D, I = index.search(np.array(query_embedding), k=5)  # Top 5 results

#print("Top 5 matching chunks:")
#for idx in I[0]:
#    print(f"\nChunk {idx}:\n{chunks[idx]}")


# Step 0: Setup - Assuming you already have 'model', 'chunks', and 'I' defined

chat_history = []  # To store conversation history

def build_context(chat_history, retrieved_chunks):
    context = "\n\n".join(retrieved_chunks)
    history_text = "\n".join([f"User: {q}\nAssistant: {a}" for q, a in chat_history])
    return f"{history_text}\n\nContext:\n{context}" if history_text else f"Context:\n{context}"

while True:
    query = input("You: ")

    if query.lower() in ["exit", "quit"]:
        print("Chatbot: Goodbye!")
        break

    # Step 1: Retrieve top matching chunks
    retrieved_chunks = [chunks[idx] for idx in I[0]]

    # Step 2: Build context including chat history
    context = build_context(chat_history, retrieved_chunks)

    # Step 3: Build the prompt
    prompt =f"""
You are SAHELI, an AI assistant dedicated to improving maternal healthcare in India, part of the Sarthak team initiative.
Your goal is to provide empathetic, clear, and accurate information, using the following context from our knowledge base.

Project Context:
{context}

When answering, ensure you:
- Use simple, understandable language.
- Prioritize the wellbeing of expectant mothers and healthcare providers.
- Stay aligned with India's Anemia Standard Treatment Guidelines and Protocols and digital health initiatives.
- Offer actionable insights or recommendations wherever possible.
- Maintain a warm and supportive tone.

User Question:
{query}

Your Helpful Answer:
"""

    # Step 4: Get response from Gemini API
    response = model.generate_content(prompt)

    # Step 5: Print the answer
    answer = response.text
    print(f"Chatbot: {answer}")

    # Step 6: Update chat history
    chat_history.append((query, answer))






